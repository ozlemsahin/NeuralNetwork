### **imports**

import sklearn.datasets
import matplotlib.pyplot as plt
import matplotlib.axes
from sklearn.preprocessing import StandardScaler
import numpy as np
import random
from sympy import *
from scipy.misc import derivative

### **create dataset** (between -1 and 1)
number of features (n) : 2 \
number of examples (m) : 100 \
number of informative (y) : 1

Y_raw = np.ones(100)
X1_raw = np.ones(100)
X2_raw = np.ones(100)
def randomForX(X_raw):
    for i in range(0,100):
        X_raw[i] = random.uniform(1.0, -1.0)
    return
randomForX(X1_raw)
randomForX(X2_raw)
print(X1_raw)
print(X2_raw)


# shape y
Y_raw = np.array(Y_raw, dtype=np.int)
print(Y_raw)
#print("old shape : " + str(Y_raw.shape))
print("new Y.shape : " + str(Y_raw.shape))
print("new x1.shape : " + str(X1_raw.shape))
print("new x2.shape : " + str(X2_raw.shape))
X1_raw = X1_raw.reshape(1,X1_raw.shape[0])# Birden fazla kez yapınca hata vermesin diye yorumdalar
X2_raw = X2_raw.reshape(1,X2_raw.shape[0])
Y_raw = Y_raw.reshape(1,Y_raw.shape[0])


#Normalize Data between [-1,1]x[-1,1] 
 #ALREADY CREATED THAT WAY

#AUFGABE 7.1.1 
#Verwenden Sie die Gerade, die durch diese zwei Punkte verläuft, 
#als die Entscheidungsgrenze ihrer Zielfunktion
#F: Punkte auf der einen Seite der Linie sollen als +1 und die anderen als −1 klassifiziert werden.

def normalizeY():
    for i in range(0,100):
       
        x1 = X1_raw[0,i]
        x2 = X2_raw[0,i]
        if(x1 > x2 or x1 == x2):
            Y_raw[0,i] = 1
        else:
            Y_raw[0,i] = -1
    return
normalizeY()

print(Y_raw)

#to understand deeply show points and classes as numbers
print("x1 : " + str(X1_raw[0,0]) )
print("x2 : " + str(X2_raw[0,0]) )
print("y : " + str(Y_raw[0,0]))


#SHOW DATASET AFTER DATASET MODIFICATION
fig , ax = plt.subplots(nrows = 1, ncols = 1)
x1 = X1_raw
x2 = X2_raw
Y = Y_raw
ax.scatter(x1, x2, marker='o', c=Y, s=25, edgecolor='b')
plt.show()

#initialize weights & learning rate & h(x)
W = np.ones(3)
W[0] = 1
W[1] = 1
W[2] = 1
learningRate = 0.001
h=0 



#calculate wTx = h(x)
def calculateWTX(W, x1, x2, i):
    return W[0]+(W[1]*x1[0,i])+(W[2]*x2[0,i])


#calculate sigmoid
def sigmoid(h, i):
    if (h < 0 ):
        return -1
    return 1


#predict    
def predictBetter(W,y, sigmoid, h, cost):    
        
        if(y < sigmoid):
            #prediction must decrease
            W = np.subtract(W , (cost * learningRate))
            
        else:
            #prediction must increase
             W = np.add(W , (cost * learningRate))
                
        return W
        

# find cost function J(h(x), y) = X_d
def findCost(W, y):
   
    x1_d = 0.0
    x2_d = 0.0
    
    #x1 turevini alınmıs halini yazdım direkt
    x1_d = -2*W[1]*(y - (W[0]+(W[1]*x1[0,i])+(W[2]*x2[0,i])))
    
    #x2 turevinin alınmıs hali
    x2_d = -2*W[2]*(y - (W[0]+(W[1]*x1[0,i])+(W[2]*x2[0,i])))
    
    
    X_d = np.ones(3)
    X_d[0] = 1# x0 ist always 1
    X_d[1] = x1_d
    X_d[2] = x2_d
    return X_d

for j in range(0,2000):
    i = j%99
    #find h(wTx=0)
    h = calculateWTX(W, x1, x2, i)
    #find sigmoid(h(wTx))
    s = sigmoid(h, i)
    #get real y value
    y = Y[0,i]
    
    #real value != estimated value
    if (y != s):
        
        X_d = findCost(W,y)
        W = predictBetter(W,y,s,h, X_d)
 
    if(j%100 == 0):
        print('w0  '+ 'w1  '+'w2                             ' + 'x1                 '+ 'x2                    '+ 'real y    '+ 'predicted  ' + 'learning rate  ')
        print(str(W)+' '+ str(x1[0,i])+'  ' + str(x2[0,i])+'         ' + str(y_val) +'          ' + str(s)+'       ' +str(learningRate))

    
    
    
    
    
    
